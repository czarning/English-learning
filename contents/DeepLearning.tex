\section{概述}
深度学习(deep learing)允许由多个处理层（(multiple processing layers)组成的计算模型来学习具有多级抽象的数据表示。这些方法显著地提高了语音识别，视觉对象识别，对象检测以及药物发现和基因组学等其他领域的先进水平。深度学习在大数据集中发现复杂的结构，通过使用反向传播算法(backpropagation algorithm,BP算法)来说明机器应该如何改变其内部参数，这些参数用于从上一层的表示中计算每一层的表示(representation)。深度卷积网络(deep convolutional nets)在处理图像、视频、语音和音频方面取得了突破，而递归网络(recurrent nets)也在文本和语音等连续数据方面开始闪光。

\section{前言}
机器学习(machine-learning)技术为现代社会的许多方面提供了动力：从网络搜索到社交网络(social networks)上的内容过滤，再到电子商务网站的推荐，而且越来越多的出现在诸如相机和智能手机等消费产品中。机器学习系统用于识别图像中的对象，把语音转换为文本，将新闻条目、帖子、或产品与用户的兴趣相匹配，并选择搜索的相关结果。这些应用程序越来越多地使用了一种叫做深度学习的技术。

传统的机器学习技术在处理原始数据的能力上受到限制。几十年来，构建一个模式识别(pattern-recognition)或机器学习系统需要细致的工程和相当多的专业知识来设计一个特征提取器，从而把原始数据(比如图像的像素值)转变成一个合适的内部表示(internal representation)或特征向量(feature vector)。学习子系统，往往是一个分类器，就可以通过这些内部表示或特征向量在输入中检测或分类模式。


表示学习(representation learning)是一套方法，它允许机器有原始数据进行输入，并且自动发现用于检测或分类所需要的表示。深度学习方法是具有多层表示的表示学习方法，通过组成简单但是非线性的模块，每个模块把一个层次的表示(从原始输入开始)转换到一个更高的、稍微抽象的层次上的表示。有了足够多这样的转换，就可以学习非常复杂的函数。对于分类任务，更高层次的表示会放大输入的各个方面，这对于辨别和抑制无关变量(irrelevant variations)是非常重要的。例如，一幅图像，以像素值数组的形式出现，在第一个表示层中学习的特性通常代表图像中特定方向和位置的边缘的存在或缺失。第二层通常通过发现边缘的特定排列来检测主题，而不考虑边缘位置的微小变化。第三层可以把图形组成更大的组合，与熟悉的对象的部分相应对应，随后的层会将对象作为这些部分的组合(detect objects as combinations of these parts)来检测。深度学习的关键方面是，这些功能层不是由人类工程师设计的：它们是从使用通用学习程序的数据中学习的。 

深度学习在解决那些多年来一直抵制人工智能社区的最佳尝试的问题上取得了重大进展。事实证明，它非常善于发现高维数据中的复杂结构，因此适用于许多科学、商业和政府领域。除了在图像识别和语音识别打破记录外，深度学习还击败了其他机器学习技术，预测潜在药物分子的活性、分析粒子加速器数据、重构脑回路并预测非编码DNA突变对基因表达和疾病的影响。也许更出人意料的是，深度学习在自然语言理解(natural language understanding)的各种任务中取得了极有希望的结果，尤其是主题分类、情感分析、问题解答和语言翻译等。

我们认为深度学习在未来会取得更多的成功，因为它不需要太多手工损伤，因此它可以很容易地利用可用的计算和数据量的增加。目前正在为深度神经网络(deep neural networks)开发的新的学习算法和架构只会加速这一进程。

\section{监督学习(supervised learning)}
最常见的机器学习形式，不管深度与否，都是监督学习。想象一下，我们要建造一个系统，能对图像进行分类，例如房子、汽车、人或宠物。我们首先收集大量关于房子、汽车、人和宠物的图片，每个都贴上它的标签。在训练过程中，机器会显示一幅图片，并以一个分数向量的形式产生输出，每个类别都有一个。我们想使希望的分类在所有分类中得分最高，但这在训练前并不会发生。我们计算一个目标函数,测量输出分数和期望模式分数之间的误差(或距离)。然后机器修改其内部可调参数，以减少这个误差。这些可调参数，通常称为权重，是实数，可以被看做是定义机器输入输出函数的'旋钮'。在一个典型的深度学习系统里，可能有数以亿计的这些可调节的重量，以及数以亿记的用来训练机器的标签示例。

为了适当地调整权重向量(weight vector)，学习算法计算一个梯度向量(gradient vector)，每一个权重，如果增加一个小的量，被用来指示权重的误差的就会增加或减少。然后将权量向量与梯度向量反向调整。

目标函数，在所有的训练样本中，可以在权重值的高维空间中被视为一种丘陵景观(hilly landscape)。负梯度向量表示这一景观中最陡的下降方向(steepest descent)，让它接近最小值，而平均输出误差是低的。

在实践中，大多数实践者使用了一个叫做SGD(stochastic gradient descent)随机梯度下降算法的程序。这个算法包含以下几个部分：给输入向量提供一些例子，计算输出和误差，计算这些例子的平均梯度值，并相应地调整权重。训练集中的许多小组例子重复这个过程，直到目标函数的平均值不再下降。它被称为随机，因为每个小的例子都给出了一个关于所有例子的平均梯度值的噪音估计。这一简单的过程通常会发现一组好的权重，而与更为复杂的优化技术相比，它的速度出奇的快。在训练之后，系统的性能是通过不同的一组被称为测试集的例子来衡量的。这是用来测试机器的泛化能力(generalization ability)：它能够对新输入给出合理的回答，而在训练过程中从未见过。

目前机器学习的许多实际应用在手工设计的特性(hand engineered feature)之上使用线性分类器。两类线性分类器计算特征向量分量(feature vector components)的加权和。如果加权和超过阈值(threshold)，输入就被分类到属于某个特定类别。

自1960年来，我们已经知道线性分类器只能把它们的输入空间划分成非常简单的区域，即由超平面(hyperplane)分隔的半空间(half spaces)。但问题是，如图像和语音识别需要输入输出函数对输入的无关变量不敏感，比如的位置、方向的变化或物体亮度，或者音调或说话口音的变化；而对于特定微小变化(比如一头白色的狼和狼一样的叫做Samoyed的白色的狗的区别)非常敏感。在像素级，两个不同姿势、不同环境的Samoyed的图像可能差别非常大，而在相同位置和相似背景下的两个Samoyed和狼的图像可能非常相似。一个线性分类器，或者其他运行在原始像素上的'浅(shallow)'分类器可能不会区分后面两个动物，而把前面两个动物归到同一类。这就是为什么浅分类器需要好的特征提取器来解决选择不变性困境(selectivity-invariance-dilemma)，这个问题产生了对图像的某些方面有选择性的表示，而这对于区分是非常重要的，但是这对于不相关的方面，比如动物的姿势，是不变的。为了使分类器更强大，可以使用通用的非线性特性，正如核方法一样(kernel methods)。但是那些由高斯核引起的泛型特性，不允许学习者在训练的示例中很好的推广。传统的选项是手工设计优秀的特征提取器，而这需要大量的工程技术和领域专业知识。但是，如果可以使用通用的学习过程自动学习好的特性，那么这一切就能够避免。这就是深度学习的关键优势。

深度学习体系结构是一个多层简单模块(multilayer stack)的栈，所有或大部分都要学习，其中很多都要计算非线性输入输出的映射。栈中的每个模块都转换其输入，以增加表示的选择性和不变性。有多个非线性层，比如深度5到20层，一个系统能够实现其输入极其复杂的功能，同时对微小的细节(用来区分Samoyed和白狼的那些细节)敏感,并且对诸如背景，姿势，亮度和周围的物体等大型无关变量不敏感。

\section{反向传播训练多层体系结构(backpropagation to train multilayer architectures)}
从最早的模式识别开始，研究人员的目标就是用可训练的多层网络来取代手动设计的特性，但尽管它很简单，直到1980年代中期这个方案才被广泛理解。事实证明，多层架构可以通过简单的随机梯度下降算法来训练。只要模块的输入和内部权重相对平稳，就可以使用反向传播过程来计算梯度。在1970年代到1980年代期间，有几个不同的组织独立地发现了这一想法，并认为是可行的。

计算一个目标函数在多层堆叠堆的权值上的梯度的反向传播过程，只不过是导数链式法则(chain rule for derivatives)的一个实际应用。关键的观点是，对于模块输入的目标函数的导数或梯度可以通过从梯度方向逆向计算，从而得到该模块的输出(或者后续模块的输入)。反向传播方程可以通过所有模块重复应用，以传播梯度，从顶部的输出(网络产生预测的地方)一直到底部(外部输入被输入)。一旦这些梯度被计算出来，就可以直接计算每个模块的权重的梯度。

深度学习的许多应用都使用前馈神经网络结构(图1)，它学习将固定大小的输入(例如图像)映射到固定大小的输出(例如每个类别的概率)。从一层到下一层，一组单元从上一层计算其输入的加权和，并通过一个非线性函数传递结果。目前，最受欢迎的非线性函数是修正线性单元(ReLU,rectified linear unit)，它只是半波整流器(half-wave rectifier)$f(z)=max(z,0)$。在过去的几十年，神经网络使用光滑的非线性函数，比如$tanh(z)$或$1\slash(1+exp(-z))$，但是ReLU通常在多层的网络中学习更快，允许没有无监督前训练(unsupervised pre-training)的深度监督网络(deep supervsied network)的训练。不在输入或输出层的单元通常叫做隐藏单元。隐藏层被视为以非线性的方式扭曲输入，从而使类别成为最后一层线性可分的(linearly separable)。

在此详细解释图1。

20世纪90年代后期，神经网络和反向传播被机器学习社区抛弃，并被计算机视觉和语音识别社区所忽视。人们普遍认为，学习有用的，多阶段的，具有少量先验知识的特征提取器是不可行的。尤其是，人们普遍认为简单的梯度下降会被困在较差的局部最小值中(poor local minima)，重量配置，没有小的变化会降低平均误差。

在实践中，较差的局部最小值问题很少是大型网络的问题。无论初始条件如何，系统几乎总是能达到非常相似的质量的解决方案。最近的理论和实证结果强烈地表明局部最小值问题不是一个严重的问题。取而代之的是，landscape是由大量的鞍点组成，梯度为零，而在大多数维度中，表面曲线向上弯曲，在其余部分向下弯曲。分析似乎表明，只有少数向下弯曲方向的鞍点存在于非常大的数字中，但是几乎所有鞍点具有非常相似的目标函数值。因此，在这些鞍点中，算法卡在哪里并不重要。

2006年，由加拿大高级研究院(CIFAR)召集的一组研究人员重新唤起了对深度前馈网络(deep feedforward networks)的兴趣。研究人员引入了无监督学习程序，可以在不需要标签数据的情况下创建多层的功能检测器。学习每一层特征检测器的目的是为了能够重构或模拟下一层特征检测器(或原始输入)的活动。使用这个重构目标，通过预先训练几层渐进复杂的特征检测器，深度网络的权重可以被初始化为可感知的值。输出单元的最后一层可以被添加到网络的顶部，而整个深度系统可以使用标准的反向传播算法进行微调(fine-tuned)。这对于识别手写数字或检测行人非常有效，尤其当标签数据非常有限的时候。

这种前向训练方法的第一个主要应用是在语音识别中，它是由于快速图像处理单元(GPU:graphics processing units)的出现而成为可能的，它方便了编程，允许研究人员更快地训练网络10倍或20倍。2009年这个方法被用来绘制从声波中提取的系数的短时间窗口，以得到可能由窗口的中心框架表示的各种各样语音片段的概率。它在一个标准的语音识别基准测试中取得了破记录的结果，该基准测试使用了一个小的词汇表，并很快被开发出来，在一个庞大的词汇表上提供了破记录的结果。到2012年，许多主要语音团队开发了来自2009年深网的各种版本，并且已经部署在Android手机上。对于更小的数据集，无监督的预先训练有助于防止过度拟合，当标记示例的数据量很小，或者在转移设置中，我们有大量的示例用于源任务，但是很少用于某些目标任务时，会导致明显更好的泛化。一旦深度学习被恢复，实践证明预先训练阶段只需要小的数据集。

然而，有一种特殊类型的深度前馈网络，它比相邻层之间完全联通的网络更容易训练和推广。这就是卷积神经网络(ConvNet:convolutional neural network)。在神经网络失宠的这段时间里，它取得了许多实际的成功，并且它最近被计算机视觉社区广泛采用。

\section{Convolutional neural network}
卷积神经网络被设计用来处理以多个数组形式出现的数据，例如，在三个颜色通道中包含像素强度的三个二维数组组成的彩色图像。许多数据形式都是以多个数组的形式出现的：一维给信号和序列，包含语言；二维给图像或声谱图；三维给视频或容积图像。在卷积神经网络中有四个主要想法，它们利用了自然信号的特性：本地连接，共享权重，池和多层的使用。

典型的卷积神经网络是由一系列阶段组成的。前面的几个阶段由两类层组成：卷积层和池层。卷积层的单位是由特征映射组织的，其中每个单元通过一组称为过滤器库(filter bank)的权重来连接上一层物特征映射中的local patches。然后这个局部加权和的结果通过一个非线性比如ReLU。特征映射中的所有单元都共享相同的过滤器库。一层中的不同特征映射使用不同的过滤器库。这种架构的原因是双重的。首先，在象图像这样的数组数据中，局部的值组通常是高度相关的，形成了很容易被检测到的独特的本地主题。其次，图像和其他信号的局部统计量对于位置是不变的。换句话说，如果一个图案出现在图像中的某个部分，它就可以出现在任何地方，因此，不同位置的单元的概念会共享相同的权重，并在数组的不同部分检测相同的模式。数学上讲，feature map执行的过滤操作是一个离散的卷积，因此得名。

虽然卷积层的作用是检测来自上一层的特征的局部连接，但池层的作用是将语义相似的特性合并为一个。由于特征形成图案的相对位置可以有一定的变化，可靠地检测出motif可以通过对每个特征位置进行粗粒化(coarsegraining)来完成。一个典型的池单元在一个feature map(或几个feature maps)中计算local patch的最大值。相邻的池单元从超过一个行或列移位的patch中获取输入，从而减少了表示的维度，并对小的移位和扭曲创建了不变性。卷积的两个或三个阶段，非线性和pooling是叠加的，紧跟着更多的卷积和全连接的层。通过一个卷积神经网络的反向传播梯度就像通过一个常规的深度网络一样简单，允许所有过滤库中的所有权重都被训练。

深层神经网络利用了许多自然信号是组合层次结构的特性，在这种属性中，通过组合较低级别的特征来获得更高层次的特征。在图像中，局部的边缘组合形成图案，图案组合成部分，部分形成物体。类似的层次结构存在于语音和文本中，从声音到电话、音素、音节、单词和句子。当上一层的元素在位置和外观上有所不同时，池允许表示有非常小的变化。

